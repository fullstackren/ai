import{_ as e,c as l,a8 as s,o as i}from"./chunks/framework.CEV-pc2D.js";const r="/assets/deepseek-dify.B957KBOX.jpg",o="/assets/dify.CCXcKuNQ.png",t="/assets/dify-github.7qrgUOKd.png",p="/assets/docker.CwynnJ3V.png",n="/assets/docker-app.n0qjz2yd.png",d="/assets/docker-compose-up-d.BG1uk868.png",h="/assets/docker-compose-up-d-2.CMyVvoeb.png",c="/assets/dify-setting.CnxuppFR.png",m="/assets/dify-ollama.Nt2K6Wxv.png",k="/assets/dify-ollama-llm.DSVszU6b.png",b="/assets/ollama.CqH4L0ZP.png",g="/assets/my-computer.BNfg6tk9.png",f="/assets/deepseek.D4L9EFu0.png",y="/assets/ollama-deepseek-r1.C7sLtDyj.png",u="/assets/ollama-deepseek-r1-32b.DovN-hRJ.png",_="/assets/ollama-deepseek-r1-32b-download.CPwUvkF8.png",D="/assets/ollama-deepseek-r1-14b-download.DCGPTtOz.png",q="/assets/ollama-bge-m3.Dk8oqVne.png",P="/assets/ollama-bge-m3-download.8DrsHPaU.png",x="/assets/dify-ollama-bge-m3.Cym_lZ2f.png",R=JSON.parse('{"title":"DeepSeek + Dify 本地部署私有化知识库","description":"","frontmatter":{},"headers":[],"relativePath":"app/deepseek-dify.md","filePath":"app/deepseek-dify.md"}'),E={name:"app/deepseek-dify.md"};function z(v,a,C,S,F,A){return i(),l("div",null,a[0]||(a[0]=[s('<h1 id="deepseek-dify-本地部署私有化知识库" tabindex="-1">DeepSeek + Dify 本地部署私有化知识库 <a class="header-anchor" href="#deepseek-dify-本地部署私有化知识库" aria-label="Permalink to &quot;DeepSeek + Dify 本地部署私有化知识库&quot;">​</a></h1><p><img src="'+r+'" alt="DeepSeek + Dify 本地部署私有化知识库" loading="lazy"></p><p>背景：公司开发产品，由于产品特性不能对外开源，其使用文档也不开源，为了更好地进行技术支持，所以需要本地部署私有化知识库。</p><p>思路：下载 Dify 应用框架，通过 Docker 环境安装项目依赖。项目启动后，下载 ollama 本地服务模型和 DeepSeek-R1 模型，最后将模型部署到 Dify 应用框架中，这样就实现了本地部署私有知识库。</p><p>那就走着~</p><h2 id="_01-dify-安装" tabindex="-1">01 <br>Dify 安装 <a class="header-anchor" href="#_01-dify-安装" aria-label="Permalink to &quot;01 &lt;br/&gt;Dify 安装&quot;">​</a></h2><h3 id="dify-下载" tabindex="-1">Dify 下载 <a class="header-anchor" href="#dify-下载" aria-label="Permalink to &quot;Dify 下载&quot;">​</a></h3><p><a href="https://dify.ai/zh" target="_blank" rel="noreferrer">Dify</a> 是一个开源的 AI 应用开发平台，它可以让开发者快速搭建自己的 AI 应用。打开官网，如下图：</p><p><img src="'+o+'" alt="Dify" loading="lazy"></p><p>点击 Github 图标，进入 Dify 项目 Github 地址，然后下载 ZIP 包，如下图：</p><p><img src="'+t+`" alt="Dify Github" loading="lazy"></p><h3 id="dify-修改配置" tabindex="-1">Dify 修改配置 <a class="header-anchor" href="#dify-修改配置" aria-label="Permalink to &quot;Dify 修改配置&quot;">​</a></h3><p>Dify ZIP 包解压后，进入 dify-main 项目根目录 <code>dify-main/docker</code>，将 <code>.env.example</code> 复制一份，命名为 <code>.env</code>，打开 <code>.env</code> 文件，滚动条滚动到底部，新增如下内容：</p><div class="language-shell vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 启用自定义模型</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">CUSTOM_MODEL_ENABLED</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">true</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 指定 ollama 的 API 地址（根据配置环境调整 IP）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">OLLAMA_API_BASE_URL</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">host.docker.internal:11434</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h2 id="_02-docker-安装" tabindex="-1">02 <br>Docker 安装 <a class="header-anchor" href="#_02-docker-安装" aria-label="Permalink to &quot;02 &lt;br/&gt;Docker 安装&quot;">​</a></h2><h3 id="docker-下载" tabindex="-1">Docker 下载 <a class="header-anchor" href="#docker-下载" aria-label="Permalink to &quot;Docker 下载&quot;">​</a></h3><p><a href="https://www.docker.com/" target="_blank" rel="noreferrer">Docker</a> 是一个开源的应用容器引擎，它可以让开发者将应用程序及其依赖项打包成一个可移植的容器，然后在任何支持 Docker 的平台上运行。如何下载 Docker，如下图：</p><p><img src="`+p+'" alt="Docker" loading="lazy"></p><h3 id="docker-安装" tabindex="-1">Docker 安装 <a class="header-anchor" href="#docker-安装" aria-label="Permalink to &quot;Docker 安装&quot;">​</a></h3><p>安装 Docker 客户端后，如下图：</p><p><img src="'+n+'" alt="" loading="lazy"></p><h3 id="项目依赖安装" tabindex="-1">项目依赖安装 <a class="header-anchor" href="#项目依赖安装" aria-label="Permalink to &quot;项目依赖安装&quot;">​</a></h3><p>进入 dify-main 项目根目录 <code>dify-main/docker</code>，执行如下命令：</p><div class="language-shell vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> compose</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> up</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>终端命令行输出如下：</p><p><img src="'+d+'" alt="终端命令行" loading="lazy"></p><p>也可以通过 Docker 客户端查看：</p><p><img src="'+h+'" alt="Docker 客户端" loading="lazy"></p><h3 id="项目查看" tabindex="-1">项目查看 <a class="header-anchor" href="#项目查看" aria-label="Permalink to &quot;项目查看&quot;">​</a></h3><p>在地址栏输入 <code>127.0.0.1/install</code>，如下图：</p><p><img src="'+c+'" alt="项目查看" loading="lazy"></p><p>点击用户中心，选择【设置】，弹框左侧菜单选择【模型供应商】，看到 Ollama 后，下面有个【添加模型】按钮，点击进入，如下图：</p><p><img src="'+m+'" alt="ollama" loading="lazy"></p><p><img src="'+k+'" alt="添加模型" loading="lazy"></p><p>所以接下来，我们要下载 ollama 和 DeepSeek-R1 模型。</p><h2 id="_03-ollama-安装" tabindex="-1">03 <br>ollama 安装 <a class="header-anchor" href="#_03-ollama-安装" aria-label="Permalink to &quot;03 &lt;br/&gt;ollama 安装&quot;">​</a></h2><h3 id="ollama-下载" tabindex="-1">ollama 下载 <a class="header-anchor" href="#ollama-下载" aria-label="Permalink to &quot;ollama 下载&quot;">​</a></h3><p><a href="https://ollama.com/" target="_blank" rel="noreferrer">ollama</a>是一个开源的本地模型服务，它可以让开发者在本地运行各种模型。如何下载 ollama，如下图：</p><p><img src="'+b+'" alt="ollama" loading="lazy"></p><h3 id="ollama-安装" tabindex="-1">ollama 安装 <a class="header-anchor" href="#ollama-安装" aria-label="Permalink to &quot;ollama 安装&quot;">​</a></h3><p>和普通应用安装一样，安装成功后，应用栏会出现一个小羊驼。</p><h2 id="_04-deepseek-r1-安装" tabindex="-1">04 <br>DeepSeek-R1 安装 <a class="header-anchor" href="#_04-deepseek-r1-安装" aria-label="Permalink to &quot;04 &lt;br/&gt;DeepSeek-R1 安装&quot;">​</a></h2><h3 id="识别下载-deepseek-r1-哪款模型" tabindex="-1">识别下载 DeepSeek-R1 哪款模型 <a class="header-anchor" href="#识别下载-deepseek-r1-哪款模型" aria-label="Permalink to &quot;识别下载 DeepSeek-R1 哪款模型&quot;">​</a></h3><p>将本地电脑配置设备信息发给 <a href="https://chat.deepseek.com/" target="_blank" rel="noreferrer">DeepSeek</a>，然后 DeepSeek 会推荐给你一个适合的模型。</p><p><img src="'+g+'" alt="本地电脑配置" loading="lazy"></p><p>提示词如下，可参考：</p><blockquote><p>我现在正在使用 ollama 部署 DeepSeek-R1 模型，但是模型分为 1.5b、7b、8b、14b、32b、70b、671b，我不知道该怎么选择适合我电脑配置模型了，我现在把我电脑的配置信息发给你，你帮我推荐一个适合我的模型。</p><p>我的电脑配置信息如下：</p><ul><li>芯片：Apple M2 Max</li><li>内存：96 GB</li></ul></blockquote><p><img src="'+f+'" alt="" loading="lazy"></p><h3 id="deepseek-r1-模型下载" tabindex="-1">DeepSeek-R1 模型下载 <a class="header-anchor" href="#deepseek-r1-模型下载" aria-label="Permalink to &quot;DeepSeek-R1 模型下载&quot;">​</a></h3><p>进入 <a href="https://ollama.com/" target="_blank" rel="noreferrer">ollama</a> 官网，点击 <a href="https://ollama.com/library/deepseek-r1" target="_blank" rel="noreferrer">DeepSeek-R1</a>, 如下图：</p><p><img src="'+y+'" alt="ollama" loading="lazy"></p><p>选择上一步你咨询 DeepSeek 推荐的模型，然后选择（我选择 32b，为了速度，我也选了 14b），然后点击右侧【复制】按钮。</p><p><img src="'+u+'" alt="ollama-32b" loading="lazy"></p><p>打开本地终端，粘贴刚刚复制的命令：</p><p><img src="'+_+'" alt="本地终端-32b" loading="lazy"></p><p><img src="'+D+'" alt="本地终端-14b" loading="lazy"></p><h3 id="embedding-向量模型下载" tabindex="-1">Embedding 向量模型下载 <a class="header-anchor" href="#embedding-向量模型下载" aria-label="Permalink to &quot;Embedding 向量模型下载&quot;">​</a></h3><p>后面会用到，这里一并下载了，这里推荐 <a href="https://ollama.com/library/bge-m3" target="_blank" rel="noreferrer">bge-m3</a> Embedding 向量模型, 点击【复制】按钮，如下图：</p><p><img src="'+q+'" alt="ollama-bge-m3" loading="lazy"></p><p>打开本地终端，粘贴刚刚复制的命令：</p><p><img src="'+P+'" alt="ollama-bge-m3-download" loading="lazy"></p><h2 id="_05-dify-配置与使用" tabindex="-1">05 <br>Dify 配置与使用 <a class="header-anchor" href="#_05-dify-配置与使用" aria-label="Permalink to &quot;05 &lt;br/&gt;Dify 配置与使用&quot;">​</a></h2><h3 id="大模型配置" tabindex="-1">大模型配置 <a class="header-anchor" href="#大模型配置" aria-label="Permalink to &quot;大模型配置&quot;">​</a></h3><h3 id="embedding-向量模型配置" tabindex="-1">Embedding 向量模型配置 <a class="header-anchor" href="#embedding-向量模型配置" aria-label="Permalink to &quot;Embedding 向量模型配置&quot;">​</a></h3><p>需要配置之前下载的 bge-m3 Embedding 向量模型，配置如下图</p><p><img src="'+x+'" alt="" loading="lazy"></p><h3 id="创建应用" tabindex="-1">创建应用 <a class="header-anchor" href="#创建应用" aria-label="Permalink to &quot;创建应用&quot;">​</a></h3><p>选择【工作室】选项卡，点击【创建空白应用】，选择【聊天助手】，如下图：</p><ol><li>填写【应用名称/图标】</li><li>填写描述</li><li>点击创建</li></ol><p>接着就进入应用空间，右上角可以看到是哪款大模型，可以切换模型。右侧【调试与预览】可以【提问】了：</p><h3 id="创建知识库" tabindex="-1">创建知识库 <a class="header-anchor" href="#创建知识库" aria-label="Permalink to &quot;创建知识库&quot;">​</a></h3><ol><li>选择【知识库】选项卡，然后导入本地的文档，进入下一步。</li><li>下方有个【索引方式】，有个【高质量（推荐）】选项初始是置灰的，但是这时候可以选中，是我们配置了 bge-m3 Embedding 向量模型，所以可以使用。</li><li>其他配置默认，然后点击【保存并处理】</li></ol><h3 id="切换到创建的应用" tabindex="-1">切换到创建的应用 <a class="header-anchor" href="#切换到创建的应用" aria-label="Permalink to &quot;切换到创建的应用&quot;">​</a></h3><ol><li>在【上下文】模块中，点击【添加】刚刚创建的知识库。</li><li>在右侧【调试与预览】进行提问，提问与知识库相关内容。</li><li>可继续追问。</li></ol>',74)]))}const w=e(E,[["render",z]]);export{R as __pageData,w as default};
